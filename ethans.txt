

import pandas as pd
import re
import os
from typing import Dict, List, Tuple
from datetime import datetime, date
import numpy as np

def load_csv_smart(filepath: str) -> pd.DataFrame:
    print(f"Attempting to load {filepath}...")
    
    try:
        df = pd.read_csv(filepath, delimiter='\t')
        if len(df.columns) == 1:
            print("Tab delimiter resulted in single column, trying comma delimiter...")
            df = pd.read_csv(filepath, delimiter=',')
        print(f"Successfully loaded with {len(df.columns)} columns")
        return df
    except Exception as e:
        # If tab fails, try comma
        print(f"Tab delimiter failed: {e}, trying comma delimiter...")
        try:
            df = pd.read_csv(filepath, delimiter=',')
            print(f"Successfully loaded with {len(df.columns)} columns")
            return df
        except Exception as e2:
            print(f"Failed to load CSV: {e2}")
            raise

def create_indicator_columns(dataframe: pd.DataFrame, 
                           column_to_search: str, 
                           keyword_map: Dict[str, List[str]], 
                           new_prefix: str) -> pd.DataFrame:
    
    print(f"\nProcessing column: {column_to_search}")
    print(f"Creating {len(keyword_map)} new features with prefix '{new_prefix}'")
    
    if column_to_search not in dataframe.columns:
        print(f"Warning: Column '{column_to_search}' not found, skipping...")
        return dataframe
    
    df_result = dataframe.copy()
    
    for suffix, search_terms in keyword_map.items():
        new_column_name = f"{new_prefix}_{suffix}"
        
        df_result[new_column_name] = 0
        
        for idx, cell_value in df_result[column_to_search].items():
            if pd.isna(cell_value) or cell_value == '':
                continue
                
            cell_text = str(cell_value).lower()
            
            # First, check if there's a direct negation of the specific compliance requirement
            direct_negation_found = False
            for term in search_terms:
                # Look for direct negations like "no KYC", "KYC not required", "optional KYC"
                direct_negation_patterns = [
                    rf'\b(no|not|does\s+not|do\s+not|don\'t|doesn\'t|never|none|without|lacks|lacking|absent|missing|exempt|exemption|waived|waiver)\s+{re.escape(term.lower())}\b',
                    rf'\b{re.escape(term.lower())}\s+(not|never|none|exempt|exemption|waived|waiver)\b',
                    rf'\b(optional|voluntary|discretionary)\s+{re.escape(term.lower())}\b',
                    rf'\b{re.escape(term.lower())}\s+(optional|voluntary|discretionary)\b'
                ]
                
                for pattern in direct_negation_patterns:
                    if re.search(pattern, cell_text):
                        # Direct negation found - mark as 0
                        df_result.loc[idx, new_column_name] = 0
                        direct_negation_found = True
                        break
                
                if direct_negation_found:
                    break
            
            # If no direct negation found, check for positive matches
            if not direct_negation_found:
                for term in search_terms:
                    pattern = r'\b' + re.escape(term.lower()) + r'\b'
                    if re.search(pattern, cell_text):
                        df_result.loc[idx, new_column_name] = 1
                        break
        
        matches = df_result[new_column_name].sum()
        print(f"  Created {new_column_name}: {matches} matches found")
    
    return df_result

def extract_launch_dates_and_create_time_features(dataframe: pd.DataFrame, 
                                                column_name: str) -> pd.DataFrame:
    """
    Extract launch dates from product launch dates column and create time decay features.
    
    Args:
        dataframe: Input DataFrame
        column_name: Name of the column containing product launch dates
        
    Returns:
        DataFrame with new time decay features
    """
    print(f"\nProcessing column: {column_name}")
    print("Creating time decay features based on product launch dates...")
    
    if column_name not in dataframe.columns:
        print(f"Warning: Column '{column_name}' not found, skipping...")
        return dataframe
    
    df_result = dataframe.copy()
    
    # Initialize time features
    df_result['Time_Earliest_Launch'] = np.nan
    df_result['Time_Latest_Launch'] = np.nan
    df_result['Time_Product_Count'] = 0
    df_result['Time_Decay_Score'] = 0.0
    df_result['Time_Innovation_Score'] = 0.0
    df_result['Time_Recent_Activity'] = 0.0
    
    # Current date for calculations (using 2025 as reference since data is from 2025)
    current_year = 2025
    current_month = 6  # June 2025 as reference
    
    for idx, cell_value in df_result[column_name].items():
        if pd.isna(cell_value) or cell_value == '':
            continue
            
        cell_text = str(cell_value).lower()
        
        # Extract all years mentioned in the text
        year_pattern = r'\b(19|20)\d{2}\b'
        years = re.findall(year_pattern, cell_text)
        
        # Extract month patterns (e.g., "Sep", "March", "Dec")
        month_pattern = r'\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\b'
        months = re.findall(month_pattern, cell_text)
        
        # Extract specific dates (e.g., "Jun 6, 2025", "August 28, 2023")
        specific_date_pattern = r'\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\s+(\d{1,2}),?\s+(19|20)\d{2}\b'
        specific_dates = re.findall(specific_date_pattern, cell_text)
        
        # Extract relative dates (e.g., "since 2017", "founded 2018")
        relative_pattern = r'\b(since|founded|launched|started|established)\s+(19|20)\d{2}\b'
        relative_dates = re.findall(relative_pattern, cell_text)
        
        # Extract ranges (e.g., "2022-2023", "2019-2020")
        range_pattern = r'\b(19|20)\d{2}[-–—]\s*(19|20)\d{2}\b'
        ranges = re.findall(range_pattern, cell_text)
        
        # Collect all years found
        all_years = []
        
        # Add years from direct matches
        all_years.extend([int(year) for year in years])
        
        # Add years from specific dates
        for month, day, year in specific_dates:
            all_years.append(int(year))
        
        # Add years from relative dates
        for _, year in relative_dates:
            all_years.append(int(year))
        
        # Add years from ranges
        for start_year, end_year in ranges:
            all_years.extend([int(start_year), int(end_year)])
        
        # Remove duplicates and sort
        all_years = sorted(list(set(all_years)))
        
        if all_years:
            earliest_year = min(all_years)
            latest_year = max(all_years)
            
            # Calculate time decay score (higher = more recent)
            # Base score: 100 for current year, decreasing by 10 per year
            base_decay_score = max(0, 100 - (current_year - latest_year) * 10)
            
            # Innovation bonus: reward for having products across multiple years
            year_span = latest_year - earliest_year + 1
            innovation_bonus = min(30, year_span * 5)  # Max 30 points for 6+ year span
            
            # Recent activity bonus: reward for very recent launches (last 2 years)
            recent_bonus = 0
            if latest_year >= current_year - 1:
                recent_bonus = 20
            
            # Product count bonus: reward for having many products
            product_count = len(years) + len(specific_dates) + len(relative_dates)
            product_bonus = min(20, product_count * 2)  # Max 20 points
            
            # Calculate final scores
            decay_score = base_decay_score + innovation_bonus + recent_bonus + product_bonus
            innovation_score = innovation_bonus + product_bonus
            recent_activity = 1 if latest_year >= current_year - 1 else 0
            
            # Store the features
            df_result.loc[idx, 'Time_Earliest_Launch'] = earliest_year
            df_result.loc[idx, 'Time_Latest_Launch'] = latest_year
            df_result.loc[idx, 'Time_Product_Count'] = product_count
            df_result.loc[idx, 'Time_Decay_Score'] = decay_score
            df_result.loc[idx, 'Time_Innovation_Score'] = innovation_score
            df_result.loc[idx, 'Time_Recent_Activity'] = recent_activity
    
    # Fill NaN values
    df_result['Time_Earliest_Launch'] = df_result['Time_Earliest_Launch'].fillna(0)
    df_result['Time_Latest_Launch'] = df_result['Time_Latest_Launch'].fillna(0)
    df_result['Time_Decay_Score'] = df_result['Time_Decay_Score'].fillna(0)
    df_result['Time_Innovation_Score'] = df_result['Time_Innovation_Score'].fillna(0)
    
    # Print summary statistics
    print(f"  Created Time_Earliest_Launch: {df_result['Time_Earliest_Launch'].nunique()} unique years")
    print(f"  Created Time_Latest_Launch: {df_result['Time_Latest_Launch'].nunique()} unique years")
    print(f"  Created Time_Product_Count: {df_result['Time_Product_Count'].sum()} total products")
    print(f"  Created Time_Decay_Score: range {df_result['Time_Decay_Score'].min():.1f} to {df_result['Time_Decay_Score'].max():.1f}")
    print(f"  Created Time_Innovation_Score: range {df_result['Time_Innovation_Score'].min():.1f} to {df_result['Time_Innovation_Score'].max():.1f}")
    print(f"  Created Time_Recent_Activity: {df_result['Time_Recent_Activity'].sum()} exchanges with recent activity")
    
    return df_result

def calculate_product_complexity_score(dataframe: pd.DataFrame, 
                                     product_columns: List[str]) -> pd.DataFrame:
    """
    Calculate a weighted product complexity score (0-10) for each exchange.
    
    The scoring system considers:
    - Product sophistication (basic vs advanced)
    - Regulatory complexity (higher regulation = higher score)
    - Product diversity and innovation
    
    Args:
        dataframe: DataFrame with product indicator columns
        product_columns: List of product feature column names
        
    Returns:
        DataFrame with new product complexity score
    """
    print(f"\nCalculating Product Complexity Scores...")
    print("Using weighted scoring system (0-10 scale)")
    
    df_result = dataframe.copy()
    
    # Product complexity weights (higher = more sophisticated/complex)
    # These weights reflect both technical complexity and regulatory implications
    PRODUCT_WEIGHTS = {
        'Product_Spot': 1.0,           # Basic trading - low complexity, low regulation
        'Product_Margin': 2.0,         # Leverage - medium complexity, medium regulation
        'Product_Staking': 1.5,        # Yield products - medium complexity, medium regulation
        'Product_P2P_OTC': 2.5,       # OTC trading - high complexity, high regulation
        'Product_Futures': 3.5,        # Derivatives - high complexity, high regulation
        'Product_Options': 4.0,        # Advanced derivatives - very high complexity, very high regulation
        'Product_Launchpad': 2.0,      # IEOs - medium complexity, medium regulation
        'Product_NFTs': 1.5,           # NFT trading - medium complexity, medium regulation
        'Product_Crypto_Loans': 3.0,   # Lending - high complexity, high regulation
    }
    
    # Initialize complexity score column
    df_result['Product_Complexity_Score'] = 0.0
    df_result['Product_Complexity_Breakdown'] = ''
    
    for idx in df_result.index:
        total_score = 0.0
        breakdown = []
        
        # Calculate weighted score for each product type
        for col in product_columns:
            if col in PRODUCT_WEIGHTS and df_result.loc[idx, col] == 1:
                weight = PRODUCT_WEIGHTS[col]
                total_score += weight
                breakdown.append(f"{col.replace('Product_', '')}: {weight:.1f}")
        
        # Apply complexity multipliers based on product combinations
        complexity_multiplier = 1.0
        
        # Bonus for having multiple product categories (diversification)
        active_products = sum([1 for col in product_columns if df_result.loc[idx, col] == 1])
        if active_products >= 5:
            complexity_multiplier *= 1.3  # 30% bonus for high diversity
        elif active_products >= 3:
            complexity_multiplier *= 1.15  # 15% bonus for medium diversity
        
        # Bonus for having advanced products (futures + options)
        has_futures = df_result.loc[idx, 'Product_Futures'] == 1 if 'Product_Futures' in product_columns else False
        has_options = df_result.loc[idx, 'Product_Options'] == 1 if 'Product_Options' in product_columns else False
        if has_futures and has_options:
            complexity_multiplier *= 1.25  # 25% bonus for having both derivatives
        
        # Bonus for having lending products (high regulatory complexity)
        has_lending = df_result.loc[idx, 'Product_Crypto_Loans'] == 1 if 'Product_Crypto_Loans' in product_columns else False
        if has_lending:
            complexity_multiplier *= 1.2  # 20% bonus for lending complexity
        
        # Apply multiplier and normalize to 0-10 scale
        final_score = min(10.0, (total_score * complexity_multiplier) / 2.0)
        
        # Store results
        df_result.loc[idx, 'Product_Complexity_Score'] = round(final_score, 2)
        df_result.loc[idx, 'Product_Complexity_Breakdown'] = ' | '.join(breakdown) if breakdown else 'No products'
    
    # Print summary statistics
    scores = df_result['Product_Complexity_Score']
    print(f"  Created Product_Complexity_Score: range {scores.min():.2f} to {scores.max():.2f}")
    print(f"  Average complexity score: {scores.mean():.2f}")
    print(f"  Score distribution:")
    print(f"    Low (0-2): {(scores <= 2).sum()} exchanges")
    print(f"    Medium (2-5): {((scores > 2) & (scores <= 5)).sum()} exchanges")
    print(f"    High (5-8): {((scores > 5) & (scores <= 8)).sum()} exchanges")
    print(f"    Very High (8-10): {(scores > 8).sum()} exchanges")
    
    return df_result

def calculate_regulatory_framework_score(dataframe: pd.DataFrame, 
                                       regulatory_columns: List[str]) -> pd.DataFrame:
    """
    Calculate a weighted regulatory framework score (0-10) for each exchange.
    
    The scoring system considers:
    - Regulatory framework strength and legal significance
    - Geographic coverage and regulatory diversity
    - Compliance with major financial jurisdictions
    - Regulatory framework maturity and enforcement
    
    Args:
        dataframe: DataFrame with regulatory indicator columns
        regulatory_columns: List of regulatory framework column names
        
    Returns:
        DataFrame with new regulatory framework score
    """
    print(f"\nCalculating Regulatory Framework Scores...")
    print("Using weighted scoring system (0-10 scale)")
    
    df_result = dataframe.copy()
    
    # Define regulatory framework weights based on legal significance
    # Higher weights = more significant regulatory frameworks
    REGULATORY_WEIGHTS = {
        'Reg_MiCA': 2.5,           # EU Markets in Crypto-Assets - Major regulatory framework
        'Reg_BitLicense': 2.0,      # New York State - Strict US regulation
        'Reg_FCA': 2.0,            # UK Financial Conduct Authority - Major EU regulator
        'Reg_FinCEN_MSB': 1.5,     # US Treasury - Federal money services regulation
        'Reg_VASP': 1.5,           # Virtual Asset Service Provider - FATF standard
        'Reg_FATF': 1.5,           # Financial Action Task Force - International AML standard
        'Reg_AUSTRAC': 1.0,        # Australian regulator - Developed market
        'Reg_FINTRAC': 1.0,        # Canadian regulator - Developed market
        'Reg_SEC_CFTC': 2.5,       # US Securities/Commodities - Major US regulators
    }
    
    # Initialize regulatory score columns
    df_result['Regulatory_Framework_Score'] = 0.0
    df_result['Regulatory_Framework_Breakdown'] = ''
    df_result['Regulatory_Jurisdiction_Count'] = 0
    df_result['Regulatory_Geographic_Diversity'] = 0.0
    
    for idx, row in df_result.iterrows():
        total_score = 0.0
        framework_details = []
        active_frameworks = 0
        
        # Calculate weighted score for each regulatory framework
        for col in regulatory_columns:
            if col in REGULATORY_WEIGHTS and row[col] == 1:
                weight = REGULATORY_WEIGHTS[col]
                total_score += weight
                active_frameworks += 1
                
                # Clean column name for display
                framework_name = col.replace('Reg_', '')
                framework_details.append(f"{framework_name}: {weight}")
        
        # Calculate geographic diversity bonus
        # More diverse regulatory coverage = higher score
        if active_frameworks >= 5:
            diversity_bonus = 1.5  # Major global presence
        elif active_frameworks >= 3:
            diversity_bonus = 1.0  # Multi-jurisdictional
        elif active_frameworks >= 2:
            diversity_bonus = 0.5  # Dual jurisdiction
        else:
            diversity_bonus = 0.0  # Single jurisdiction
        
        # Calculate regulatory maturity bonus
        # Exchanges with more comprehensive regulation get bonus points
        if total_score >= 8.0:
            maturity_bonus = 1.0  # Highly regulated
        elif total_score >= 5.0:
            maturity_bonus = 0.5  # Well regulated
        elif total_score >= 2.0:
            maturity_bonus = 0.25  # Moderately regulated
        else:
            maturity_bonus = 0.0  # Minimally regulated
        
        # Apply bonuses
        final_score = total_score + diversity_bonus + maturity_bonus
        
        # Normalize to 0-10 scale
        # Maximum possible score: 18.5 (all frameworks + max bonuses)
        normalized_score = min(10.0, (final_score / 18.5) * 10.0)
        
        # Store results
        df_result.loc[idx, 'Regulatory_Framework_Score'] = round(normalized_score, 2)
        df_result.loc[idx, 'Regulatory_Framework_Breakdown'] = ' | '.join(framework_details) if framework_details else 'No regulatory frameworks'
        df_result.loc[idx, 'Regulatory_Jurisdiction_Count'] = active_frameworks
        df_result.loc[idx, 'Regulatory_Geographic_Diversity'] = round(diversity_bonus, 2)
    
    # Print summary statistics
    scores = df_result['Regulatory_Framework_Score']
    print(f"  Created Regulatory_Framework_Score: range {scores.min():.2f} to {scores.max():.2f}")
    print(f"  Average regulatory score: {scores.mean():.2f}")
    print(f"  Score distribution:")
    print(f"    Low (0-2): {(scores <= 2).sum()} exchanges")
    print(f"    Medium (2-5): {((scores > 2) & (scores <= 5)).sum()} exchanges")
    print(f"    High (5-8): {((scores > 5) & (scores <= 8)).sum()} exchanges")
    print(f"    Very High (8-10): {(scores > 8).sum()} exchanges")
    
    return df_result

def main():
    print("=" * 70)
    print("Cryptocurrency Exchange Data Preprocessor V2")
    print("Keyword-based feature engineering (avoids feature explosion)")
    print("=" * 70)
    
    GEO_KEYWORD_MAP = {
        'USA': ['usa', 'u.s.', 'united states', 'america'],
        'UK': ['uk', 'united kingdom'],
        'EU': ['eu', 'europe', 'eea', 'european union'],
        'China': ['china', 'mainland china'],
        'Hong_Kong': ['hong kong', 'hk'],
        'Singapore': ['singapore'],
        'Canada': ['canada'],
        'Japan': ['japan', 'jp'],
        'South_Korea': ['south korea', 'korea'],
        'Australia': ['australia', 'au'],
        'UAE': ['uae', 'united arab emirates', 'dubai'],
        'Switzerland': ['switzerland'],
        'Seychelles': ['seychelles'],
        'BVI': ['bvi', 'british virgin islands'],
        'Cayman_Islands': ['cayman islands'],
        'LATAM': ['latam', 'latin america', 'mexico', 'brazil', 'argentina', 'colombia', 'chile'],
        'Africa': ['africa', 'south africa', 'nigeria'],
        'Turkey': ['turkey'],
        'Russia': ['russia'],
        'India': ['india'],
        'USA_Restricted': ['restricted in us', 'barred from us', 'excludes us', 'excluding us', 
                          'excluding united states', 'except us', 'except united states'],
        'China_Restricted': ['restricted in china', 'excludes china', 'excluding china', 
                            'except china', 'excluding mainland china'],
    }
    
    PRODUCT_KEYWORD_MAP = {
        'Spot': ['spot trading', 'spot'],
        'Futures': ['futures', 'perpetual', 'derivatives'],
        'Options': ['options'],
        'Margin': ['margin', 'leverage'],
        'Staking': ['staking', 'earn'],
        'P2P_OTC': ['p2p', 'otc', 'over-the-counter', 'over the counter'],
        'NFTs': ['nft', 'nfts', 'non-fungible'],
        'Launchpad': ['launchpad', 'ieo', 'initial exchange offering'],
        'Crypto_Loans': ['loans', 'lending', 'borrow'],
    }
    
    REGULATION_KEYWORD_MAP = {
        'MiCA': ['mica', 'micar', 'markets in crypto'],
        'BitLicense': ['bitlicense', 'bit license'],
        'FCA': ['fca', 'financial conduct authority'],
        'FinCEN_MSB': ['fincen', 'msb', 'money service business'],
        'VASP': ['vasp', 'virtual asset service provider'],
        'FATF': ['fatf', 'financial action task force'],
        'AUSTRAC': ['austrac'],
        'FINTRAC': ['fintrac'],
        'SEC_CFTC': ['sec', 'cftc', 'securities and exchange', 'commodity futures'],
    }
    
    COMPLIANCE_KEYWORD_MAP = {
        'AML_KYC': ['aml', 'kyc', 'anti-money laundering', 'know your customer', 'customer verification', 'identity verification', 'real-name', 'real name'],
        'Travel_Rule': ['travel rule', 'fatf travel', 'cross-border reporting', 'wire transfer rule'],
        'Transaction_Monitoring': ['transaction monitoring', 'transaction surveillance', 'suspicious activity', 'fraud detection', 'ai-based fraud'],
        'Sanctions_Screening': ['sanctions screening', 'ofac', 'sanctions compliance', 'embargo compliance'],
        'Disclosure_Requirements': ['disclosure', 'financial disclosure', 'regulatory disclosure', 'transparency', 'proof of reserves', 'proof-of-reserves'],
        'Audit_Reporting': ['audit', 'external audit', 'regulatory reporting', 'fiu reporting', 'str reporting', 'suspicious transaction'],
        'Tiered_KYC': ['tiered kyc', 'kyc tiers', 'verification levels', 'tiered verification', 'basic kyc', 'advanced kyc'],
        'Proof_of_Reserves': ['proof of reserves', 'proof-of-reserves', 'reserve verification', 'cold storage', 'hot wallet', 'wallet segregation'],
    }
    
    GOVERNANCE_KEYWORD_MAP = {
        'Public_Disclosure': ['public', 'publicly', 'disclosed', 'transparent', 'transparency', 'governance', 'ownership', 'structure', 'board', 'directors', 'ceo', 'founder', 'founded by', 'incorporated', 'inc', 'ltd', 'llc', 'corp', 'corporation', 'limited', 'company', 'partnership', 'joint venture', 'subsidiary', 'parent company', 'holding company', 'venture capital', 'private equity', 'ipo', 'initial public offering', 'stock exchange', 'nasdaq', 'nyse', 'listed', 'publicly traded', 'shareholders', 'stakeholders', 'investors', 'backed by', 'funded by', 'series a', 'series b', 'series c', 'funding round', 'valuation', 'market cap', 'enterprise value'],
    }
    
    data_file = "Crypto Exchange Regulation Research Master Spreadsheet (2).csv"
    if not os.path.exists(data_file):
        print(f"Error: File {data_file} not found!")
        return
    
    try:
        df_original = load_csv_smart(data_file)
        print(f"\nOriginal DataFrame shape: {df_original.shape}")
        print(f"Columns: {list(df_original.columns)[:10]}...")  # Show first 10 columns
    except Exception as e:
        print(f"Failed to load data: {e}")
        return
    
    identifier_columns = ['Crypto Exchange']
    if 'Link to doc with full analysis' in df_original.columns:
        identifier_columns.append('Link to doc with full analysis')
    
    df_clean = df_original[identifier_columns].copy()
    
    if 'Country/Region(s) of Operation' in df_original.columns:
        df_with_geo = create_indicator_columns(
            df_original, 
            'Country/Region(s) of Operation',
            GEO_KEYWORD_MAP,
            'Geo'
        )
        geo_columns = [col for col in df_with_geo.columns if col.startswith('Geo_')]
        for col in geo_columns:
            df_clean[col] = df_with_geo[col]
    
    if 'Products Offered (spot, futures, options, etc.)' in df_original.columns:
        df_with_products = create_indicator_columns(
            df_original,
            'Products Offered (spot, futures, options, etc.)',
            PRODUCT_KEYWORD_MAP,
            'Product'
        )
        product_columns = [col for col in df_with_products.columns if col.startswith('Product_')]
        for col in product_columns:
            df_clean[col] = df_with_products[col]
        
        # Calculate product complexity scores
        df_with_complexity = calculate_product_complexity_score(
            df_with_products,
            product_columns
        )
        complexity_columns = ['Product_Complexity_Score', 'Product_Complexity_Breakdown']
        for col in complexity_columns:
            df_clean[col] = df_with_complexity[col]
    
    reg_columns = [
        'Key Regulatory Frameworks (MiCA, BitLicense, etc.)',
        'Regulatory Exposure (licenses, jurisdictions)'
    ]
    
    combined_reg_text = pd.Series('', index=df_original.index)
    for col in reg_columns:
        if col in df_original.columns:
            combined_reg_text = combined_reg_text + ' ' + df_original[col].fillna('')
    
    df_temp = df_original.copy()
    df_temp['Combined_Regulatory_Text'] = combined_reg_text
    
    df_with_regulations = create_indicator_columns(
        df_temp,
        'Combined_Regulatory_Text',
        REGULATION_KEYWORD_MAP,
        'Reg'
    )
    
    reg_feature_columns = [col for col in df_with_regulations.columns if col.startswith('Reg_')]
    for col in reg_feature_columns:
        df_clean[col] = df_with_regulations[col]
    
    # Calculate regulatory framework scores
    df_with_reg_scores = calculate_regulatory_framework_score(
        df_with_regulations,
        reg_feature_columns
    )
    reg_score_columns = ['Regulatory_Framework_Score', 'Regulatory_Framework_Breakdown', 'Regulatory_Jurisdiction_Count', 'Regulatory_Geographic_Diversity']
    for col in reg_score_columns:
        df_clean[col] = df_with_reg_scores[col]
    
    if 'Compliance Requirements (AML/KYC, disclosure, etc.)' in df_original.columns:
        df_with_compliance = create_indicator_columns(
            df_original,
            'Compliance Requirements (AML/KYC, disclosure, etc.)',
            COMPLIANCE_KEYWORD_MAP,
            'Compliance'
        )
        compliance_columns = [col for col in df_with_compliance.columns if col.startswith('Compliance_')]
        for col in compliance_columns:
            df_clean[col] = df_with_compliance[col]
    
    if 'Ownership & Governance Structure' in df_original.columns:
        df_with_governance = create_indicator_columns(
            df_original,
            'Ownership & Governance Structure',
            GOVERNANCE_KEYWORD_MAP,
            'Governance'
        )
        governance_columns = [col for col in df_with_governance.columns if col.startswith('Governance_')]
        for col in governance_columns:
            df_clean[col] = df_with_governance[col]
    
    if 'Product Launch Dates' in df_original.columns:
        df_with_time_features = extract_launch_dates_and_create_time_features(
            df_original,
            'Product Launch Dates'
        )
        time_columns = [col for col in df_with_time_features.columns if col.startswith('Time_')]
        for col in time_columns:
            df_clean[col] = df_with_time_features[col]
    
    if 'Exchange Size (Large/Medium/Small)' in df_original.columns:
        print("\nProcessing Exchange Size (simple categorical)...")
        size_dummies = pd.get_dummies(
            df_original['Exchange Size (Large/Medium/Small)'], 
            prefix='Size',
            dummy_na=False
        )
        for col in size_dummies.columns:
            df_clean[col] = size_dummies[col]
        print(f"  Created {len(size_dummies.columns)} size categories")
    
    output_file = 'crypto_exchanges_analysis_ready_with_improved_negation.csv'
    df_clean.to_csv(output_file, index=False)
    
    print("\n" + "=" * 70)
    print("PROCESSING COMPLETE - SUMMARY")
    print("=" * 70)
    print(f"Original DataFrame shape: {df_original.shape}")
    print(f"Final DataFrame shape: {df_clean.shape}")
    print(f"\nFeatures created by category:")
    print(f"  Geographic features: {len([c for c in df_clean.columns if c.startswith('Geo_')])}")
    print(f"  Product features: {len([c for c in df_clean.columns if c.startswith('Product_')])} (including complexity scores)")
    print(f"  Regulatory features: {len([c for c in df_clean.columns if c.startswith('Reg_')])} (including framework scores)")
    print(f"  Compliance features: {len([c for c in df_clean.columns if c.startswith('Compliance_')])}")
    print(f"  Governance features: {len([c for c in df_clean.columns if c.startswith('Governance_')])}")
    print(f"  Time features: {len([c for c in df_clean.columns if c.startswith('Time_')])}")
    print(f"  Size features: {len([c for c in df_clean.columns if c.startswith('Size_')])}")
    print(f"\nTotal new features: {len(df_clean.columns) - len(identifier_columns)}")
    print(f"\nOutput saved to: {output_file}")
    print("=" * 70)
    

if __name__ == "__main__":
    main()